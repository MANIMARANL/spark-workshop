<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

    <title>Apache Spark 2 Workshop | Introduction to Apache Spark</title>

    <meta name="description" content="Apache Spark 2 Workshop | Introduction to Apache Spark">
    <meta name="author" content="Jacek Laskowski">

    <link rel="stylesheet" href="reveal.js/css/reveal.css">
    <link rel="stylesheet" href="reveal.js/css/theme/beige.css" id="theme">

    <!-- Theme used for syntax highlighting of code -->
    <link rel="stylesheet" href="reveal.js/lib/css/zenburn.css">

    <!-- Jacek: custom formatting -->
    <link rel="stylesheet" href="revealjs-css/jacek.css">

    <!-- Printing and PDF exports -->
    <script>
      var link = document.createElement('link');
      link.rel = 'stylesheet';
      link.type = 'text/css';
      link.href = window.location.search.match(/print-pdf/gi) ? 'reveal.js/css/print/pdf.css' : 'reveal.js/css/print/paper.css';
      document.getElementsByTagName('head')[0].appendChild(link);
    </script>
  </head>

  <body>
    <div class="reveal">

      <div class="footer">
        <footer style="font-size: small;">&copy;<a href="https://medium.com/@jaceklaskowski">Jacek Laskowski</a> 2019 / <a href="https://twitter.com/jaceklaskowski">@JacekLaskowski</a> / jacek@japila.pl</footer>
      </div>

      <div class="slides">

        <section class="intro" data-transition="zoom" id="home">
          <p>
            <img width="15%" style="background:none; border:none; box-shadow:none;" data-src="images/spark-logo.png">
            <img width="7%" src="images/jacek_laskowski_20141201_512px.png" style="border: 0">
          </p>
          <h2>Introduction to</h2>
          <h1 style="font-size: 3.44em;">Apache Spark</h1>
          <h3>Apache Spark 2.4.0</h3>

          <h4 style="font-size: smaller;">
            <a href="https://twitter.com/jaceklaskowski">@jaceklaskowski</a> / <a href="http://stackoverflow.com/users/1305344/jacek-laskowski">StackOverflow</a> / <a href="https://github.com/jaceklaskowski">GitHub</a>
            <br>
            The "Internals" Books: <a href="https://bit.ly/mastering-apache-spark">Apache Spark</a> / <a href="https://bit.ly/mastering-spark-sql">Spark SQL</a> / <a href="https://bit.ly/spark-structured-streaming">Spark Structured Streaming</a>
          </h4>
        </section>

        <section id="agenda" style="font-size: 90%" data-markdown>
          <textarea data-template>
            ## Agenda

            1. [Apache Spark](#/intro)
          </textarea>
        </section>

        <section id="intro" style="font-size: 90%" data-markdown>
          <textarea data-template>
            <small>From <a href="http://spark.apache.org/">the official documentation</a> of Apache Spark:</small>
            <img width="65%" style="background:none; border:none; box-shadow:none;" data-src="images/about-spark.png" />
          </textarea>
        </section>

        <section>
          <p>
            <img width="100%" src="images/spark-platform.png" style="border: 0">
          </p>
        </section>

        <section>
          <section id="why-spark" style="font-size: 90%" data-markdown>
            <textarea data-template>
              <h2>Why Apache Spark <small>(1 of 5)</small></h2>

              Quoting the official documentation in <a href="http://spark.apache.org/docs/latest/index.html">Spark Overview</a>:

              _"**Apache Spark** is a fast and general-purpose cluster computing system. It provides high-level APIs in Java, Scala, Python and R, and an optimized engine that supports general execution graphs. It also supports a rich set of higher-level tools including Spark SQL for SQL and structured data processing, MLlib for machine learning, GraphX for graph processing, and Spark Streaming."_

            </textarea>
          </section>

          <section style="font-size: 90%" data-markdown>
            <textarea data-template>
              <h2>Why Apache Spark <small>(2 of 5)</small></h2>

              Quoting myself from <a href="https://jaceklaskowski.gitbooks.io/mastering-apache-spark/content/spark-overview.html">The Internals of Apache Spark</a>:

              _"**Apache Spark** is an open-source distributed general-purpose cluster computing framework with (mostly) in-memory data processing engine that can do ETL, analytics, machine learning and graph processing on large volumes of data at rest (batch processing) or in motion (streaming processing) with rich concise high-level APIs for the programming languages: Scala, Python, Java, R, and SQL."_

            </textarea>
          </section>

          <section style="font-size: 90%" data-markdown>
            <textarea data-template>
              <h2>Why Apache Spark <small>(3 of 5)</small></h2>

              Quoting Matei Zaharia (the author of Apache Spark) in <a href="https://youtu.be/49Hr5xZyTEA">Introduction to AmpLab Spark Internals</a> video:

              _"One of the Spark project goals was to deliver a platform that supports a very wide array of <b>diverse workflows</b> - not only MapReduce <b>batch jobs</b> (there were available in Hadoop already at that time), but also <b>iterative computations</b> like graph algorithms or Machine Learning. [...] And also <b>different scales of workloads</b> from sub-second interactive jobs to jobs that run for many hours."_

            </textarea>
          </section>

          <section style="font-size: 90%" data-markdown>
            <textarea data-template>
              ## Why Apache Spark <small>(4 of 5)</small>

              * General-purpose distributed computing framework
              * Open source under [the Apache License, Version 2.0](https://en.wikipedia.org/wiki/Apache_License)
              * High-level APIs for batch and stream processing
              * Support for Scala, Python, Java, R and SQL
              * Supports general execution graphs (described in RDD API)
              * High-level specialized engines
                * Spark SQL for batch ETL and analytics
                * Spark MLlib for machine learning
                * Spark Structured Streaming for stream processing
            </textarea>
          </section>

          <section style="font-size: 80%" data-markdown>
            <textarea data-template>
              ## Why Apache Spark <small>(5 of 5)</small>

              * RDD (Resilient Distributed Dataset)
                * Distributed Parallel Collection API for Scala
                * Description (metadata) of distributed computations
                * Data locality
              * DataFrame and Dataset
                * Python's pandas for Distributed Environment
                * Type-safe (Scala and Java)
              * Interactive exploration and exploratory analytics
                * Ad hoc queries (command line and notebooks)
              * Batch and highly iterative workloads
              * Rich data source support
              * Support for various cluster managers
                * Hadoop YARN, Kubernetes, Spark Standalone, Apache Mesos
            </textarea>
          </section>
        </section>

        <section id="spark-core">
          <h2>Spark Core</h2>
          <ul>
            <li><b>RDD</b> is the main abstraction of Apache Spark</li>
            <li>
              <b>R</b>esilient
            </li>
            <li>
              <b>D</b>istributed
            </li>
            <li>
              <b>D</b>ataset
            </li>
            <li>
              In-Memory, Immutable, Lazy Evaluated, Partitioned, Cacheable, Parallel, Typed
            </li>
            <li>
              <b>CAUTION:</b> Don't use directly. Too low-level. Leave it alone until you really <i>really</i> want it. Even then, think twice.
            </li>
          </ul>
        </section>

        <section id="spark-sql">
          <h2>Spark SQL</h2>
          <ul>
            <li>
              Distributed SQL Engine for Structured Data Processing
            </li>
            <li>
              <b>SQL</b> Interface
            </li>
            <li><b>Dataset</b> and <b>DataFrame</b></li>
            <ul>
              <li>
                DataFrame is a type alias for Dataset of Rows.
              </li>
            </ul>
            <li>
              Query DSL
            </li>
            <li>
              Hive Support
            </li>
            <li>
              <b>UDF</b> - User-Defined Functions
            </li>
            <li>
              Aggregation and Window Operators
            </li>
          </ul>
        </section>

        <section id="spark-mllib">
          <h2>Spark MLlib</h2>
          <ul>
            <li>
              Large-Scale Distributed Machine Learning
            </li>
            <li>
              "Making practical machine learning scalable and easy"
            </li>
            <li>
               DataFrame-based API
            </li>
            <li><b>Pipeline API</b> for designing, evaluating, and tuning machine learning pipelines</li>
            <li>Classification, Regression, Clustering, Recommendation, Collaborative Filtering, ...</li>
            <li>
              Model Import / Export
            </li>
          </ul>
        </section>

        <section id="spark-structured-streaming">
          <h2>Spark Structured Streaming</h2>
        </section>

        <section id="tools">
          <h2>Tools</h2>
          <ul>
            <li>
              <b>spark-shell</b> - an interactive shell for Apache Spark
            </li>
            <li>
              <b>spark-submit</b> - a tool to manage Spark applications (i.e. submit, kill, status)
            </li>
            <li>
              <b>spark-sql</b> - an interactive shell for Spark SQL and Hive queries
            </li>
            <li>
              <b>web UI</b> - a web interface to monitor Spark computations
            </li>
            <li>
              <a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.package">Scala API for Apache Spark</a>
            </li>
            <li>
              <a href="http://spark.apache.org/docs/latest/api/python/index.html">Spark Python API Docs</a>
            </li>
          </ul>
        </section>

        <section style="text-align: left" data-markdown id="questions">
          <textarea data-template>
            # Questions?

            * Read [The Internals of Apache Spark](https://bit.ly/mastering-apache-spark)
            * Read [The Internals of Kafka Streams](http://bit.ly/mastering-kafka-streams)
            * Follow [@jaceklaskowski](https://twitter.com/jaceklaskowski) on twitter
            * Upvote [my questions and answers on StackOverflow](http://stackoverflow.com/users/1305344/jacek-laskowski)
            * Find me on [Medium](https://medium.com/@jaceklaskowski)
          </textarea>
        </section>

      </div>
    </div>

    <script src="reveal.js/lib/js/head.min.js"></script>
    <script src="reveal.js/js/reveal.js"></script>

    <script>
      // More info about config & dependencies:
      // - https://github.com/hakimel/reveal.js#configuration
      // - https://github.com/hakimel/reveal.js#dependencies
      Reveal.initialize({
        controls: true,
        progress: true,
        history: true,
        center: true,
        slideNumber: true,

        transition: 'slide', // none/fade/slide/convex/concave/zoom

        menu: {
          markers: true,
          openSlideNumber: true
        },
        dependencies: [
          { src: 'reveal.js/lib/js/classList.js', condition: function () { return !document.body.classList; } },
          { src: 'reveal.js/plugin/markdown/marked.js' },
          { src: 'reveal.js/plugin/markdown/markdown.js' },
          { src: 'reveal.js/plugin/zoom-js/zoom.js', async: true },
          { src: 'reveal.js/plugin/notes/notes.js', async: true },
          { src: 'reveal.js/plugin/highlight/highlight.js', async: true, callback: function () { hljs.initHighlightingOnLoad(); } }
        ]
      });
    </script>
    <script>
      (function (i, s, o, g, r, a, m) {
        i['GoogleAnalyticsObject'] = r; i[r] = i[r] || function () {
          (i[r].q = i[r].q || []).push(arguments)
        }, i[r].l = 1 * new Date(); a = s.createElement(o),
          m = s.getElementsByTagName(o)[0]; a.async = 1; a.src = g; m.parentNode.insertBefore(a, m)
      })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');

      ga('create', 'UA-45999426-3', 'auto');
      ga('send', 'pageview');

    </script>
  </body>
</html>
