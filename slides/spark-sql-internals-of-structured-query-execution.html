<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

    <title>Apache Spark™ Workshop | Spark SQL | The Internals of Structured Query Execution</title>

    <meta name="description" content="Apache Spark™ Workshop | Spark SQL | The Internals of Structured Query Execution">
    <meta name="author" content="Jacek Laskowski">

    <link rel="stylesheet" href="reveal.js/css/reveal.css">
    <link rel="stylesheet" href="reveal.js/css/theme/beige.css" id="theme">

    <!-- Theme used for syntax highlighting of code -->
    <link rel="stylesheet" href="reveal.js/lib/css/zenburn.css">

    <!-- Jacek: custom formatting -->
    <link rel="stylesheet" href="revealjs-css/jacek.css">

    <!-- Printing and PDF exports -->
    <script>
      var link = document.createElement('link');
      link.rel = 'stylesheet';
      link.type = 'text/css';
      link.href = window.location.search.match(/print-pdf/gi) ? 'reveal.js/css/print/pdf.css' : 'reveal.js/css/print/paper.css';
      document.getElementsByTagName('head')[0].appendChild(link);
    </script>
  </head>

  <body>
    <div class="reveal">

      <div class="footer">
        <footer style="font-size: small;">
          &copy; <a href="https://medium.com/@jaceklaskowski">Jacek Laskowski</a> 2019 / <a href="https://twitter.com/jaceklaskowski">@JacekLaskowski</a>
          / jacek@japila.pl
        </footer>
      </div>

      <div class="slides">

        <section class="intro" data-transition="zoom" id="home">
          <p>
            <img width="12%" style="background:none; border:none; box-shadow:none;" data-src="images/spark-logo.png">
            <img width="6%" src="images/jacek_laskowski_20141201_512px.png" style="border: 0">
          </p>
          <h1 style="font-size: 2.77em;">The Internals of <br /> Structured Query Execution</h1>
          <h3>Apache Spark 2.4.4 / Spark SQL</h3>

          <hr />
          <h4 style="font-size: smaller;">
            <a href="https://twitter.com/jaceklaskowski">@jaceklaskowski</a> / <a href="https://stackoverflow.com/users/1305344/jacek-laskowski">StackOverflow</a> / <a href="https://github.com/jaceklaskowski">GitHub</a>
            <br>
            The "Internals" Books: <a href="https://bit.ly/apache-spark-internals">Apache Spark</a> / <a href="https://bit.ly/spark-sql-internals">Spark SQL</a> / <a href="https://bit.ly/spark-structured-streaming">Spark Structured Streaming</a>
          </h4>
        </section>

        <section id="agenda" data-markdown>
          <textarea data-template>
            ## Agenda

            1. [Structured Queries](#/structured-queries)
            1. [Query Languages](#/query-languages)
            1. [Project Catalyst](#/catalyst)
            1. [Query Execution](#/query-execution)
            1. [SparkSessionExtensions](#/SparkSessionExtensions)
            1. [Spark Core's RDD](#/spark-core-rdd)
            1. [Debugging Query Execution](#/debugging)
            1. [Whole-Stage Java Code Generation](#/whole-stage-codegen)
            1. [Tungsten Execution Backend](#/tungsten)
          </textarea>
        </section>

        <section>
          <section id="structured-queries">
            <h2>Structured Queries</h2>
            <ol>
              <li><b>Structured query</b> is a query over data that is described by a schema
                <ul>
                  <li>In other words, data has a <b>structure</b></li>
                </ul>
              </li>
              <li><b>Schema</b> is a tuple of three elements:
                <ol>
                  <li>Name</li>
                  <li>Type</li>
                  <li>Nullability</li>
                </ol>
              </li>
              <li>Remember SQL? It's a <b>S</b>tructured <b>Q</b>uery <b>L</b>anguage</li>
            </ol>
          </section>
          <section id="structured-queries-example">
            <h2>Examples of Structured Queries</h2>
            <p>
            <pre style="margin-left: 0px;"><code style="width: 900px;" class="lang-scala hljs">
    // Dataset API for Scala
    spark.table("t1")
      .join(spark.table("t2"), "id")
      .where($"id" % 2 === 0)

    // SQL
    sql("""SELECT t1.* FROM t1
           INNER JOIN t2 ON t1.id = t2.id
           WHERE t1.id % 2 = 0""")
            </code></pre>
            </p>
          </section>
          <section id="why-structure-important">
            <h2>Why is Structure important?</h2>
            <ol>
              <li><b>Storage</b></li>
              <li>The less space to store large data sets the better</li>
              <li><b>Tungsten</b> project <small>(More in the following slides)</small></li>
            </ol>
          </section>
        </section>

        <section>
          <section id="query-languages">
            <h2>Query Languages in Spark SQL</h2>
            <ol>
              <li>High-level declarative query languages:</li>
                <ul>
                  <li>Good ol' <b>SQL</b></li>
                  <li>Untyped row-based <b>DataFrame</b></li>
                  <li>Typed <b>Dataset</b></li>
                </ul>
              </li>
              <li>Another "axis" are programming languages
                <ul>
                  <li>Scala, Java, Python, R</li>
                </ul>
              </li>
            </ol>
          </section>
          <section id="sql">
            <h2>SQL</h2>
            <ol>
              <li><b>S</b>tructured <b>Q</b>uery <b>L</b>anguage</li>
              <li>SQL 92</li>
              <li>Hive QL</li>
              <li>ANTLR grammar <small>(from Presto)</small></li>
            </ol>
          </section>
          <section id="dataframe">
            <h2>DataFrames (and Schema)</h2>
            <ol>
              <li><b>DataFrame</b> &mdash; a collection of rows with a schema</li>
              <li><b>Row</b> and <b>RowEncoder</b></li>
              <li>DataFrame = <b>Dataset[Row]</b>
                <ul>
                  <li>Type alias in Scala</li>
                </ul>
              </li>
              <li>Switch to <a href="https://bit.ly/spark-sql-internals">The Internals of Spark SQL</a>
                <ul>
                  <li><a href="https://jaceklaskowski.gitbooks.io/mastering-spark-sql/content/spark-sql-DataFrame.html">DataFrame</a></li>
                </ul>
              </li>
            </ol>
          </section>
          <section id="dataset">
            <h2>Datasets (and Encoders)</h2>
            <ol>
              <li><b>Dataset</b> - strongly-typed API for working with structured data in Scala</li>
              <li><b>Encoder</b> - Serialization and deserialization API
                <ul>
                  <li>Converts a JVM object of type T to and from an <b>InternalRow</b></li>
                  <li><b>ExpressionEncoder</b> is the one and only implementation</li>
                </ul>
              </li>
              <li>Switch to <a href="https://bit.ly/spark-sql-internals">The Internals of Spark SQL</a>
                <ul>
                  <li><a href="https://jaceklaskowski.gitbooks.io/mastering-spark-sql/content/spark-sql-Dataset.html">Dataset</a></li>
                  <li><a href="https://jaceklaskowski.gitbooks.io/mastering-spark-sql/content/spark-sql-Encoder.html">Encoders &mdash; Internal Row Converters</a></li>
                </ul>
              </li>
            </ol>
          </section>
        </section>

        <section>
          <section id="catalyst">
            <h2>Project Catalyst</h2>
            <ol>
              <li><b>Catalyst</b> &mdash; Tree Manipulation Framework</li>
              <li><b>TreeNode</b> with child nodes
                <ul>
                  <li>Children are TreeNodes again</li>
                  <li>Recursive data structure</li>
                </ul>
              </li>
              <li><b>Rules</b> to manipulate TreeNodes
                <ul>
                  <li>Rules executed sequentially</li>
                  <li>Loops supported</li>
                </ul>
              </li>
            </ol>
          </section>
          <section id="catalyst-node">
            <h2>TreeNode</h2>
            <ol>
              <li><b>TreeNode</b> is a node with child nodes (<b>children</b>)
                <ul>
                  <li>Recursive data structure</li>
                  <li>Builds a tree of TreeNodes</li>
                </ul>
              </li>
            </ol>
            <p>
            <img width="80%" style="background:none; border:none; box-shadow:none;" data-src="images/sparksql-catalyst-treenode.png">
            </p>
            <i>What a nice class definition in Scala, isn't it?</i>
          </section>
          <section id="catalyst-treenodes">
            <h2>TreeNodes</h2>
            <p>
            <img style="background:none; border:none; box-shadow:none;" data-src="images/sparksql-catalyst-treenodes.png">
            </p>
          </section>
          <section id="queryplan" style="font-size: 90%;">
            <h2>Query Plans</h2>
            <img width="90%" style="background:none; border:none; box-shadow:none;" data-src="images/sparksql-queryplans.png">
            <ol>
              <li><b>QueryPlan</b> &mdash; the base node for relational operators</li>
              <li><b>LogicalPlan</b> &mdash; the base logical query plan</li>
              <li><b>SparkPlan</b> &mdash; the base physical query plan</li>
              <li>Switch to <a href="https://bit.ly/spark-sql-internals">The Internals of Spark SQL</a>
                <ul>
                  <li><a href="https://jaceklaskowski.gitbooks.io/mastering-spark-sql/content/spark-sql-LogicalPlan.html">LogicalPlan</a></li>
                  <li><a href="https://jaceklaskowski.gitbooks.io/mastering-spark-sql/content/spark-sql-SparkPlan.html">SparkPlan</a></li>
                </ul>
              </li>
            </ol>
          </section>
          <section id="treenode-sparkplan">
            <h2>Example of TreeNode (SparkPlan)</h2>
            <img width="33%" style="background:none; border:none; box-shadow:none;" data-src="images/treenode-sparkplan.png">
          </section>
          <section id="treenode-physical-plan-explain">
            <h2>Example of TreeNode (explain output)</h2>
            <img style="background:none; border:none; box-shadow:none;" data-src="images/treenode-physical-plan-explain.png">
          </section>
          <section id="expression" style="font-size: 90%;">
            <h2>Catalyst Expressions</h2>
            <ol>
              <li><b>Expression</b> &mdash; an executable node (in a Catalyst tree)</li>
              <li>Evaluates to a JVM object given InternalRow</li>
            </ol>
            <p>
            <pre><code>
    def eval(input: InternalRow = null): Any
            </code></pre>
            </p>
          </section>
          <section id="catalyst-rule">
            <h2>Rule</h2>
            <p>
            <img width="80%" style="background:none; border:none; box-shadow:none;" data-src="images/sparksql-catalyst-rule.png">
            </p>
          </section>
          <section id="catalyst-rule-example" style="font-size: 90%;">
            <h2>Example: RemoveAllHints Logical Rule</h2>
            <p>
            <img width="80%" style="background:none; border:none; box-shadow:none;" data-src="images/sparksql-catalyst-rule-RemoveAllHints.png">
            </p>
          </section>
          <section id="catalyst-ruleexecutor">
            <h2>RuleExecutor</h2>
            <p>
            <img width="80%" style="background:none; border:none; box-shadow:none;" data-src="images/sparksql-catalyst-ruleexecutor.png">
            </p>
          </section>
        </section>

        <section>
          <section id="query-execution">
            <h2>Query Execution</h2>
            <ol>
              <li><b>QueryExecution</b> - the heart of any structured query
                <ul>
                  <li>Structured Query Execution Pipeline</li>
                  <li>Execution Phases</li>
                </ul>
              </li>
              <li>Use <b>Dataset.explain</b> to know the plans</li>
              <li>Use <b>Dataset.queryExecution</b> to access the phases</li>
              <li><b>QueryExecution.toRdd</b> to generate a <b>RDD</b> <small>(more later)</small> </li>
            </ol>
          </section>
          <section id="query-execution-pipeline">
            <h2>QueryExecution Pipeline</h2>
            <img width="70%" style="background:none; border:none; box-shadow:none;" data-src="images/sparksql-queryexecution-pipeline.png">
          </section>
          <section id="spark-analyzer">
            <h2>Spark Analyzer</h2>
            <img width="70%" style="background:none; border:none; box-shadow:none;" data-src="images/sparksql-analyzer.png">
            <ol>
              <li><b>Spark Analyzer</b> - Validates logical query plans and expressions</li>
              <li>RuleExecutor of <b>LogicalPlans</b></li>
              <li>Once a structured query has passed Spark Analyzer the query will be executed</li>
              <li>Allows for registering new rules</li>
            </ol>
          </section>
          <section id="catalyst-optimizer">
            <h2>Catalyst (Base) Optimizer</h2>
            <img width="80%" style="background:none; border:none; box-shadow:none;" data-src="images/spark-sql-catalyst-optimizer.png">
            <ol>
              <li><b>Catalyst Query Optimizer</b>
                <ul>
                  <li>Base of query optimizers</li>
                  <li>Optimizes logical plans and expressions</li>
                  <li>Predefined batches of rules</li>
                </ul>
              </li>
              <li>Cost-Based Optimization</li>
              <li>Allows for <b>extendedOperatorOptimizationRules</b></li>
              <li>Switch to <a href="https://bit.ly/spark-sql-internals">The Internals of Spark SQL</a>
                <ul>
                  <li><a href="https://jaceklaskowski.gitbooks.io/mastering-spark-sql/content/spark-sql-Optimizer.html">Optimizer</a></li>
                </ul>
              </li>
            </ol>
          </section>
          <section id="spark-optimizer">
            <h2>Spark Logical Optimizer</h2>
            <img width="70%" style="background:none; border:none; box-shadow:none;" data-src="images/spark-sql-spark-optimizer.png">
            <ol>
              <li><b>Spark Logical Optimizer</b>
                <ul>
                  <li>Custom Catalyst query optimizer</li>
                  <li>Adds new optimization rules</li>
                </ul>
              </li>
              <li>Defines <b>extension points</b>
                <ul>
                  <li>preOptimizationBatches</li>
                  <li>postHocOptimizationBatches</li>
                  <li>ExperimentalMethods.extraOptimizations</li>
                </ul>
              </li>
            </ol>
          </section>
          <section id="spark-planner">
            <h2>Spark Planner</h2>
            <img width="80%" style="background:none; border:none; box-shadow:none;" data-src="images/sparksql-planner.png">
            <ol>
              <li><b>Spark Planner</b>
                <ul>
                  <li>Plans optimized logical plans to physical plans</li>
                </ul>
              </li>
              <li><b>At least</b> one physical plan for any given logical plan
                <ul>
                  <li><b>Exactly one</b> in Spark 2.3</li>
                </ul>
              </li>
              <li>Defines <b>extension point</b>
                <ul>
                  <li>ExperimentalMethods.extraStrategies</li>
                </ul>
              </li>
            </ol>
          </section>
          <section id="spark-preparations">
            <h3>Spark Physical Optimizer (Preparations)</h3>
            <img width="80%" style="background:none; border:none; box-shadow:none;" data-src="images/sparksql-preparations.png">
            <ol>
              <li>Physical query optimizations (aka <b>preparations</b> rules)
                <ul>
                  <li>Whole-Stage Code Generation</li>
                  <li>Reuse Exchanges and Subqueries</li>
                  <li>EnsureRequirements (with Bucketing)</li>
                </ul>
              </li>
              <li>Note the type <b>Rule[SparkPlan]</b>
                <ul>
                  <li>Rules take a SparkPlan and produce a SparkPlan</li>
                </ul>
              </li>
            </ol>
          </section>
        </section>

        <section>
          <section id="SparkSessionExtensions" style="font-size: 90%">
            <h2>SparkSessionExtensions</h2>
            <ul>
              <li>Hooks and Extension Points</li>
              <li>Customize a <b>SparkSession</b> with user-defined extensions
                <ul>
                  <li>Custom query execution rules</li>
                  <li>Custom relational entity parser</li>
                </ul>
              </li>
              <li>injectCheckRule</li>
              <li>injectOptimizerRule</li>
              <li>injectParser</li>
              <li>injectPlannerStrategy</li>
              <li>injectPostHocResolutionRule</li>
              <li>injectResolutionRule</li>
              <li>Use <b>Builder.withExtensions</b> or <b>spark.sql.extensions</b></li>
            </ul>
          </section>
          <section id="SparkSessionExtensions-example">
            <h2>SparkSessionExtensions Example</h2>
            <pre style="margin-left: 20px;"><code style="width: 900px;" class="lang-scala hljs">
        import org.apache.spark.sql.SparkSession
        val spark = SparkSession
          .builder
          .withExtensions { extensions =>
            extensions.injectResolutionRule { session =>
              ...
            }
            extensions.injectOptimizerRule { session =>
              ...
            }
          }
          .getOrCreate
            </code></pre>
          </section>
        </section>

        <section>
          <section id="spark-core-rdd" style="font-size: 90%;">
            <h2>Spark Core's RDD</h2>
            <ol>
              <li>Anything executable on Spark has to be a RDD
                <ul>
                  <li>...or simply a <b>job</b> (of <b>stages</b>)</li>
                </ul>
              </li>
              <li><b>RDD</b> describes a <b>distributed computation</b></li>
              <li>RDDs live in <b>SparkContext</b> on the <b>driver</b></li>
              <li>RDD is composed of <b>partitions</b> and <b>compute</b> method
                <ul>
                  <li>Partitions are parts of your data</li>
                  <li>compute method is the code you wrote</li>
                </ul>
              </li>
              <li>Partitions become tasks at runtime</li>
              <li><b>Task</b> is your code executed on a part of data</li>
              <li>Tasks are executed on <b>executors</b></li>
            </ol>
          </section>
          <section id="rdd-partitions-action-stage-tasks">
            <h2>(No) RDD at runtime</h2>
            <img width="70%" style="background:none; border:none; box-shadow:none;" data-src="images/stage-tasks.png">
          </section>
          <section id="rdd-lineage">
            <h2>RDD Lineage</h2>
            <ol>
              <li><b>RDD Lineage</b> shows RDD with dependencies</li>
              <li>RDD lineage is a graph of computation steps</li>
              <li><b>RDD.toDebugString</b></li>
            </ol>
          </section>
          <section id="query-execution-pipeline-again">
            <h2>QueryExecution Pipeline...Again</h2>
            <img width="70%" style="background:none; border:none; box-shadow:none;" data-src="images/sparksql-queryexecution-pipeline.png">
          </section>
          <section id="structured-query-as-rdd">
            <h2>Structured Queries and RDDs</h2>
            <ol>
              <li><b>QueryExecution.toRdd</b> - the very last phase in a query execution</li>
              <li>Spark SQL generates an RDD to execute a structured query</li>
              <li>Spark SQL uses higher-level structured queries to express RDD-based distributed computations
                <ul>
                  <li>RDD API is like assembler (or JVM bytecode)</li>
                  <li>Dataset API is like Scala or Python</li>
                </ul>
              </li>
            </ol>
          </section>
        </section>

        <section id="debugging">
          <h2>Debugging Query Execution</h2>
          <ol>
            <li><b>debug</b> Scala package object with <b>debug</b> and <b>debugCodegen</b>
              <pre><code>
  import org.apache.spark.sql.execution.debug._

  val q = spark.range(10).where('id === 4)
  q.debug

  val q = sql("SELECT * FROM RANGE(10) WHERE id = 4")
  q.debugCodegen
              </code></pre>
            </li>
            <li>Switch to <a href="https://bit.ly/spark-sql-internals">The Internals of Spark SQL</a>
              <ul>
                <li><a href="https://jaceklaskowski.gitbooks.io/mastering-spark-sql/content/spark-sql-debugging-execution.html">Debugging Query Execution</a></li>
              </ul>
            </li>
          </ol>
        </section>
        <section>
          <section id="whole-stage-codegen">
            <h2>Whole-Stage Java Code Generation</h2>
            <ol>
              <li><b>Whole-Stage CodeGen</b> physical query optimization</li>
              <li>Collapses a query plan tree into a single optimized function</li>
              <li>Applied to structured queries through <b>CollapseCodegenStages</b> physical optimization
                <ul>
                  <li><b>spark.sql.codegen.wholeStage</b> internal property</li>
                </ul>
              </li>
              <li>Switch to <a href="https://bit.ly/spark-sql-internals">The Internals of Spark SQL</a>
                <ul>
                  <li><a href="https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-whole-stage-codegen.html">Whole-Stage Java Code Generation (Whole-Stage CodeGen)</a></li>
                </ul>
              </li>
            </ol>
          </section>
          <section id="whole-stage-codegen-webui">
            <h2>Whole-Stage Code Gen in Web UI</h2>
            <img width="40%" style="background:none; border:none; box-shadow:none;" data-src="images/treenode-sparkplan.png">
          </section>
          <section id="whole-stage-codegen-CollapseCodegenStages-rule" style="font-size: 90%;">
            <h2>CollapseCodegenStages Optimization Rule</h2>
            <p>
            <img width="85%" style="background:none; border:none; box-shadow:none;" data-src="images/sparksql-CollapseCodegenStages.png">
            </p>
          </section>
        </section>
        <section id="tungsten" style="font-size: 90%">
          <h2>Tungsten Execution Backend</h2>
          <ol>
            <li><b>Tungsten Execution Backend</b> (aka <i>Project Tungsten</i>)
              <ul>
                <li>Optimizing Spark jobs for CPU and memory efficiency</li>
                <li>It is assumed that network and disk I/O are not performance bottlenecks</li>
              </ul>
            </li>
            <li><b>InternalRow</b> data abstraction</li>
            <li><b>UnsafeRow</b>
              <ul>
                <li>Backed by raw memory</li>
                <li>Uses <b>sun.misc.Unsafe</b></li>
              </ul>
            </li>
            <li>Switch to <a href="https://bit.ly/spark-sql-internals">The Internals of Spark SQL</a>
              <ul>
                <li><a href="https://jaceklaskowski.gitbooks.io/mastering-spark-sql/content/spark-sql-tungsten.html">Tungsten Execution Backend</a></li>
              </ul>
            </li>
          </ol>
        </section>

        <section id="recap">
          <h2>Recap</h2>
          <ol>
            <li><a href="#/structured-queries">Structured Queries</a></li>
            <li><a href="#/query-languages">Query Languages</a></li>
            <li><a href="#/catalyst">Project Catalyst</a></li>
            <li><a href="#/query-execution">Query Execution</a></li>
            <li><a href="#/SparkSessionExtensions">SparkSessionExtensions</a></li>
            <li><a href="#/spark-core-rdd">Spark Core's RDD</a></li>
            <li><a href="#/debugging">Debugging Query Execution</a></li>
            <li><a href="#/whole-stage-codegen">Whole-Stage Java Code Generation</a></li>
            <li><a href="#/tungsten">Tungsten Execution Backend</a></li>
          </ol>
        </section>

        <section style="text-align: left" data-markdown id="questions">
          <textarea data-template>
            # Questions?

            * Read [The Internals of Apache Spark](https://bit.ly/apache-spark-internals)
            * Read [The Internals of Spark SQL](https://bit.ly/spark-sql-internals)
            * Read [The Internals of Spark Structured Streaming](https://bit.ly/spark-structured-streaming)
            * Follow [@jaceklaskowski](https://twitter.com/jaceklaskowski) on twitter
            * Upvote [my questions and answers on StackOverflow](http://stackoverflow.com/users/1305344/jacek-laskowski)
          </textarea>
        </section>

      </div>
    </div>

    <script src="reveal.js/lib/js/head.min.js"></script>
    <script src="reveal.js/js/reveal.js"></script>

    <script>
      // More info about config & dependencies:
      // - https://github.com/hakimel/reveal.js#configuration
      // - https://github.com/hakimel/reveal.js#dependencies
      Reveal.initialize({
        controls: true,
        progress: true,
        history: true,
        center: true,
        slideNumber: true,

        transition: 'slide', // none/fade/slide/convex/concave/zoom

        menu: {
          markers: true,
          openSlideNumber: true
        },
        dependencies: [
          { src: 'reveal.js/lib/js/classList.js', condition: function () { return !document.body.classList; } },
          { src: 'reveal.js/plugin/markdown/marked.js' },
          { src: 'reveal.js/plugin/markdown/markdown.js' },
          { src: 'reveal.js/plugin/zoom-js/zoom.js', async: true },
          { src: 'reveal.js/plugin/notes/notes.js', async: true },
          { src: 'reveal.js/plugin/highlight/highlight.js', async: true, callback: function () { hljs.initHighlightingOnLoad(); } }
        ]
      });
    </script>
    <script>
      (function (i, s, o, g, r, a, m) {
        i['GoogleAnalyticsObject'] = r; i[r] = i[r] || function () {
          (i[r].q = i[r].q || []).push(arguments)
        }, i[r].l = 1 * new Date(); a = s.createElement(o),
          m = s.getElementsByTagName(o)[0]; a.async = 1; a.src = g; m.parentNode.insertBefore(a, m)
      })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');

      ga('create', 'UA-45999426-3', 'auto');
      ga('send', 'pageview');

    </script>
  </body>
</html>
